{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vince\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dropout, Flatten, Convolution2D, MaxPooling2D, Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    num_classes = len(list(os.scandir(path)))\n",
    "    data = load_files(path)\n",
    "    files = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']), num_classes)\n",
    "    return files, targets\n",
    "\n",
    "FILES_DIR = os.path.abspath(\"D:\\\\Data\\\\Dermatology\\\\train\")\n",
    "train_files, ytrain = load_dataset(FILES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2000/2000 [10:53<00:00,  3.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "ksize = 5\n",
    "scalar = 0.4\n",
    "ROWS = int(128 * scalar)\n",
    "COLS = int(96 * scalar)\n",
    "\n",
    "CHANNELS = 3\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(ROWS, COLS))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "xtrain = paths_to_tensor(train_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3, 51, 38)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not currently needed; we have our validation data in a separate folder already\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# xtrain, xtest, ytrain, ytest = train_test_split(train_tensors, train_targets, test_size=0.1)\n",
    "\n",
    "# Make sure to edit the NN so that\n",
    "# input_shape=(ROWS, COLS, CHANNELS) is accurate\n",
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    num_classes = len(list(os.scandir(path)))\n",
    "    data = load_files(path)\n",
    "    files = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']), num_classes)\n",
    "    return files, targets\n",
    "\n",
    "FILES_DIR = os.path.abspath(\"D:\\\\Data\\\\Dermatology\\\\valid\")\n",
    "valid_files, yval = load_dataset(FILES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 150/150 [01:54<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "# ksize = 5\n",
    "# ROWS = 128\n",
    "# COLS = 96\n",
    "\n",
    "# CHANNELS = 3\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(ROWS, COLS))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "xval = paths_to_tensor(valid_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard CNN\n",
    "\n",
    "A pretty run-of-the-mill CNN that uses convolutions and max pooling. Performs poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 51, 38)        2432      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 51, 38)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 25, 19)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 25, 19)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 12, 9)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 128, 12, 9)        204928    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 128, 6, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 256, 6, 4)         819456    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 256, 3, 2)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3072)              4721664   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 6,295,939\n",
      "Trainable params: 6,295,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "rlr = ReduceLROnPlateau(factor=.01, patience=2)\n",
    "es = EarlyStopping(monitor='val_acc', patience=0)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, kernel_size=(ksize, ksize), padding='same',\n",
    "                        input_shape=(CHANNELS, ROWS, COLS),\n",
    "                        activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(Convolution2D(64, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=[2,2], data_format=\"channels_first\"))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(64, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=[2,2], data_format=\"channels_first\"))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(128, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=[2,2], data_format=\"channels_first\"))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(256, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=[2,2], data_format=\"channels_first\",))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(3072, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5199999984105428"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(xtrain, ytrain, epochs=200, batch_size=128,\n",
    "          callbacks=[es, rlr], validation_data=(xval, yval), verbose=0)\n",
    "\n",
    "loss, acc = model.evaluate(xval, yval)\n",
    "\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 64, 51, 38)        4864      \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 64, 51, 38)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 64, 25, 19)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 128, 25, 19)       204928    \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 128, 25, 19)       409728    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 128, 12, 9)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 128, 12, 9)        409728    \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 256, 12, 9)        819456    \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 256, 12, 9)        1638656   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 256, 6, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 512, 6, 4)         3277312   \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 512, 6, 4)         6554112   \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 512, 6, 4)         6554112   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 512, 3, 2)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 512, 3, 2)         6554112   \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 512, 3, 2)         6554112   \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 512, 3, 2)         6554112   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 512, 1, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 40,294,595\n",
      "Trainable params: 40,294,595\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(64, kernel_size=(ksize, ksize), padding='same',\n",
    "                        input_shape=(CHANNELS, ROWS, COLS),\n",
    "                        activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(Convolution2D(64, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=[2,2], data_format=\"channels_first\"))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(128, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(Convolution2D(128, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=[2,2], data_format=\"channels_first\"))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(128, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(Convolution2D(256, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(Convolution2D(256, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=[2,2], data_format=\"channels_first\"))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(512, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(Convolution2D(512, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(Convolution2D(512, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=[2,2], data_format=\"channels_first\"))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(512, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(Convolution2D(512, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(Convolution2D(512, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=[2,2], data_format=\"channels_first\"))\n",
    "# model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# if i == True:\n",
    "#     model.add(Dense(512, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5199999984105428"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(xtrain, ytrain, epochs=200, batch_size=128,\n",
    "          callbacks=[es, rlr], validation_data=(xval, yval), verbose=0)\n",
    "\n",
    "loss, acc = model.evaluate(xval, yval)\n",
    "\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG with center normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "activation_7 (Activation)    (None, 3, 64, 48)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 64, 64, 48)        4864      \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 64, 64, 48)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 64, 32, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 128, 32, 24)       204928    \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 128, 32, 24)       409728    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 128, 16, 12)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 128, 16, 12)       409728    \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 256, 16, 12)       819456    \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 256, 16, 12)       1638656   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 256, 8, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 512, 8, 6)         3277312   \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 512, 8, 6)         6554112   \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 512, 8, 6)         6554112   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 512, 4, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 512, 4, 3)         6554112   \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 512, 4, 3)         6554112   \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 512, 4, 3)         6554112   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 512, 2, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 40,818,883\n",
      "Trainable params: 40,818,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation\n",
    "\n",
    "def center_normalize(x):\n",
    "    return (x-K.mean(x))/K.std(x)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Activation(activation=center_normalize, input_shape=(CHANNELS, ROWS, COLS),))\n",
    "\n",
    "model.add(Convolution2D(64, kernel_size=(ksize, ksize), padding='same',\n",
    "                        input_shape=(CHANNELS, ROWS, COLS),\n",
    "                        activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(Convolution2D(64, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=[2,2], data_format=\"channels_first\"))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(128, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(Convolution2D(128, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=[2,2], data_format=\"channels_first\"))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(128, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(Convolution2D(256, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(Convolution2D(256, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=[2,2], data_format=\"channels_first\"))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(512, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(Convolution2D(512, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(Convolution2D(512, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=[2,2], data_format=\"channels_first\"))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(512, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(Convolution2D(512, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(Convolution2D(512, kernel_size=(ksize, ksize), padding='same', activation='relu',\n",
    "                        data_format=\"channels_first\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=[2,2], data_format=\"channels_first\"))\n",
    "# model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# if i == True:\n",
    "#     model.add(Dense(512, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5199999984105428"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(xtrain, ytrain, epochs=200, batch_size=16,\n",
    "          callbacks=[es, rlr], validation_data=(xval, yval), verbose=0)\n",
    "\n",
    "loss, acc = model.evaluate(xval, yval)\n",
    "\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others\n",
    "\n",
    "## 2 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b - 0s 286us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7511111442248026"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "rlr = ReduceLROnPlateau(factor=.01, patience=2)\n",
    "es = EarlyStopping(monitor='val_acc', patience=5)\n",
    "\n",
    "\n",
    "adam = Optimizer=Adam(lr=0.001)\n",
    "\n",
    "def center_normalize(x):\n",
    "    return (x-K.mean(x))/K.std(x)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Activation(activation=center_normalize, input_shape=(CHANNELS, ROWS, COLS),))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(xtrain, ytrain, epochs=200, batch_size=64,\n",
    "          callbacks=[es, rlr], validation_data=(xval, yval), verbose=0)\n",
    "\n",
    "loss, acc = model.evaluate(xval, yval)\n",
    "\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b - 0s 320us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7422222526868184"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "rlr = ReduceLROnPlateau(factor=.01, patience=2)\n",
    "es = EarlyStopping(monitor='val_acc', patience=5)\n",
    "\n",
    "\n",
    "adam = Optimizer=Adam(lr=0.001)\n",
    "\n",
    "def center_normalize(x):\n",
    "    return (x-K.mean(x))/K.std(x)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Activation(activation=center_normalize, input_shape=(CHANNELS, ROWS, COLS),))\n",
    "\n",
    "# convolutional layer\n",
    "model.add(Convolution2D(16, kernel_size=(ksize, ksize), activation='relu',\n",
    "                        data_format='channels_first', padding='same'))\n",
    "#pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2,2), data_format='channels_first'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "model.fit(xtrain, ytrain, epochs=200, batch_size=64,\n",
    "          callbacks=[es, rlr], validation_data=(xval, yval), verbose=0)\n",
    "\n",
    "loss, acc = model.evaluate(xval, yval)\n",
    "\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "activation_9 (Activation)    (None, 3, 51, 38)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 16, 51, 38)        1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 16, 25, 19)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 32, 25, 19)        12832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 32, 12, 9)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 3456)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2048)              7079936   \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 7,356,643\n",
      "Trainable params: 7,356,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "rlr = ReduceLROnPlateau(factor=.01, patience=2)\n",
    "es = EarlyStopping(monitor='val_acc', patience=5)\n",
    "\n",
    "\n",
    "adam = Optimizer=Adam(lr=0.001)\n",
    "\n",
    "def center_normalize(x):\n",
    "    return (x-K.mean(x))/K.std(x)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Activation(activation=center_normalize, input_shape=(CHANNELS, ROWS, COLS),))\n",
    "\n",
    "# convolutional layer\n",
    "model.add(Convolution2D(16, kernel_size=(ksize, ksize), activation='relu',\n",
    "                        data_format='channels_first', padding='same'))\n",
    "#pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2,2), data_format='channels_first'))\n",
    "\n",
    "# convolutional layer\n",
    "model.add(Convolution2D(32, kernel_size=(ksize, ksize), activation='relu',\n",
    "                        data_format='channels_first', padding='same'))\n",
    "#pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2,2), data_format='channels_first'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Relu \n",
    "model.add(Dense(2048, activation='relu'))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(3))\n",
    "\n",
    "model.summary()\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "model.fit(xtrain, ytrain, epochs=200, batch_size=64,\n",
    "          callbacks=[es, rlr], validation_data=(xval, yval), verbose=0)\n",
    "\n",
    "loss, acc = model.evaluate(xval, yval)\n",
    "\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# yval_binary = np.where(np.argmax(yval, axis=1) == 1, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to predict a single image\n",
    "\n",
    "Just for illustrative purposes, I grabbed an image from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = os.path.abspath(\"D:\\\\Data\\\\Dermatology\\\\train\\\\nevus\\\\ISIC_0000001.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "ksize = 5\n",
    "scalar = 0.5\n",
    "ROWS = int(128 * scalar)\n",
    "COLS = int(96 * scalar)\n",
    "\n",
    "CHANNELS = 3\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(ROWS, COLS))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img):\n",
    "    list_of_tensors = [path_to_tensor(img)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "img = paths_to_tensor(img).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 64, 48)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.806889e-05, 9.999280e-01, 3.877733e-06]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It trained on this image, so don't take the predictions as an\n",
    "# indication of the model's accuracy.\n",
    "model.predict_proba(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAAD8CAYAAAAys+slAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEvAJ4nAk4BPE4UjAJ4jDkYBPEIcjgf8PvD/C7Z65JLcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21700147da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show image\n",
    "\n",
    "image = np.squeeze(img)\n",
    "image = np.swapaxes(image, 0, 1)\n",
    "image = np.swapaxes(image, 2, 1)\n",
    "nev = plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
